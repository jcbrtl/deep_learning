{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from model_mlp import MLPModule\n",
    "from min_max_mlp_data import MinMaxDiffModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "data = MinMaxDiffModule(50000)\n",
    "model = MLPModule()\n",
    "trainer = Trainer(check_val_every_n_epoch=5, gpus=1, max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | model        | MLP              | 353 K \n",
      "1 | loss         | CrossEntropyLoss | 0     \n",
      "2 | val_accuracy | Accuracy         | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▍         | 14/317 [00:00<00:01, 240.68it/s, loss=3.518, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushaj/miniconda3/envs/dl/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/kushaj/miniconda3/envs/dl/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 312/312 [00:01<00:00, 308.16it/s, loss=2.805, v_num=6]\n",
      "Epoch 4: : 317it [00:01, 302.95it/s, loss=2.805, v_num=6, val_loss=2.84, acc=0.098]\n",
      "Epoch 9: 100%|██████████| 312/312 [00:01<00:00, 257.57it/s, loss=2.706, v_num=6, val_loss=2.84, acc=0.098]\n",
      "Epoch 9: : 317it [00:01, 255.52it/s, loss=2.706, v_num=6, val_loss=2.82, acc=0.101]                       \n",
      "Epoch 14: 100%|██████████| 312/312 [00:01<00:00, 265.18it/s, loss=2.649, v_num=6, val_loss=2.82, acc=0.101]\n",
      "Epoch 14: : 317it [00:01, 263.72it/s, loss=2.649, v_num=6, val_loss=2.88, acc=0.101]                       \n",
      "Epoch 19: 100%|██████████| 312/312 [00:01<00:00, 262.46it/s, loss=2.574, v_num=6, val_loss=2.88, acc=0.101]\n",
      "Epoch 19: : 317it [00:01, 261.15it/s, loss=2.574, v_num=6, val_loss=2.93, acc=0.086]                       \n",
      "Epoch 24: 100%|██████████| 312/312 [00:01<00:00, 263.43it/s, loss=2.440, v_num=6, val_loss=2.93, acc=0.086]\n",
      "Epoch 24: : 317it [00:01, 262.14it/s, loss=2.440, v_num=6, val_loss=3.14, acc=0.0902]                      \n",
      "Epoch 29: 100%|██████████| 312/312 [00:01<00:00, 299.20it/s, loss=2.212, v_num=6, val_loss=3.14, acc=0.0902]\n",
      "Epoch 29: : 317it [00:01, 296.79it/s, loss=2.212, v_num=6, val_loss=3.45, acc=0.0882]                       \n",
      "Epoch 34: 100%|██████████| 312/312 [00:01<00:00, 259.78it/s, loss=2.016, v_num=6, val_loss=3.45, acc=0.0882]\n",
      "Epoch 34: : 317it [00:01, 258.58it/s, loss=2.016, v_num=6, val_loss=3.88, acc=0.0808]                       \n",
      "Epoch 39: 100%|██████████| 312/312 [00:01<00:00, 260.11it/s, loss=1.785, v_num=6, val_loss=3.88, acc=0.0808]\n",
      "Epoch 39: : 317it [00:01, 258.91it/s, loss=1.785, v_num=6, val_loss=4.41, acc=0.0804]                       \n",
      "Epoch 44: 100%|██████████| 312/312 [00:01<00:00, 271.62it/s, loss=1.555, v_num=6, val_loss=4.41, acc=0.0804]\n",
      "Epoch 44: : 317it [00:01, 270.19it/s, loss=1.555, v_num=6, val_loss=4.98, acc=0.0822]                       \n",
      "Epoch 49: 100%|██████████| 312/312 [00:01<00:00, 255.76it/s, loss=1.373, v_num=6, val_loss=4.98, acc=0.0822]\n",
      "Epoch 49: : 317it [00:01, 250.17it/s, loss=1.373, v_num=6, val_loss=5.68, acc=0.0792]                       \n",
      "Epoch 54: 100%|██████████| 312/312 [00:01<00:00, 267.05it/s, loss=0.679, v_num=6, val_loss=5.68, acc=0.0792]\n",
      "Epoch 54: : 317it [00:01, 265.44it/s, loss=0.679, v_num=6, val_loss=7.86, acc=0.0788]                       \n",
      "Epoch 59: 100%|██████████| 312/312 [00:01<00:00, 251.15it/s, loss=0.549, v_num=6, val_loss=7.86, acc=0.0788]\n",
      "Epoch 59: : 317it [00:01, 250.16it/s, loss=0.549, v_num=6, val_loss=9.41, acc=0.0838]                       \n",
      "Epoch 64: 100%|██████████| 312/312 [00:01<00:00, 251.72it/s, loss=0.486, v_num=6, val_loss=9.41, acc=0.0838]\n",
      "Epoch 64: : 317it [00:01, 250.10it/s, loss=0.486, v_num=6, val_loss=10.6, acc=0.0838]                       \n",
      "Epoch 69: 100%|██████████| 312/312 [00:01<00:00, 267.37it/s, loss=0.392, v_num=6, val_loss=10.6, acc=0.0838]\n",
      "Epoch 69: : 317it [00:01, 265.78it/s, loss=0.392, v_num=6, val_loss=11.5, acc=0.0874]                       \n",
      "Epoch 74: 100%|██████████| 312/312 [00:01<00:00, 262.86it/s, loss=0.434, v_num=6, val_loss=11.5, acc=0.0874]\n",
      "Epoch 74: : 317it [00:01, 257.07it/s, loss=0.434, v_num=6, val_loss=12.2, acc=0.0848]                       \n",
      "Epoch 79: 100%|██████████| 312/312 [00:01<00:00, 267.01it/s, loss=0.323, v_num=6, val_loss=12.2, acc=0.0848]\n",
      "Epoch 79: : 317it [00:01, 264.63it/s, loss=0.323, v_num=6, val_loss=12.9, acc=0.0876]                       \n",
      "Epoch 84: 100%|██████████| 312/312 [00:01<00:00, 277.72it/s, loss=0.292, v_num=6, val_loss=12.9, acc=0.0876]\n",
      "Epoch 84: : 317it [00:01, 275.93it/s, loss=0.292, v_num=6, val_loss=13.4, acc=0.0826]                       \n",
      "Epoch 89: 100%|██████████| 312/312 [00:01<00:00, 275.76it/s, loss=0.266, v_num=6, val_loss=13.4, acc=0.0826]\n",
      "Epoch 89: : 317it [00:01, 273.12it/s, loss=0.266, v_num=6, val_loss=13.7, acc=0.0794]                       \n",
      "Epoch 94: 100%|██████████| 312/312 [00:01<00:00, 261.72it/s, loss=0.259, v_num=6, val_loss=13.7, acc=0.0794]\n",
      "Epoch 94: : 317it [00:01, 259.51it/s, loss=0.259, v_num=6, val_loss=13.7, acc=0.0818]                       \n",
      "Epoch 99: 100%|██████████| 312/312 [00:01<00:00, 272.94it/s, loss=0.311, v_num=6, val_loss=13.7, acc=0.0818]\n",
      "Epoch 99: : 317it [00:01, 266.11it/s, loss=0.311, v_num=6, val_loss=14.5, acc=0.0808]                       \n",
      "Epoch 104: 100%|██████████| 312/312 [00:01<00:00, 256.81it/s, loss=0.015, v_num=6, val_loss=14.5, acc=0.0808]\n",
      "Epoch 104: : 317it [00:01, 255.34it/s, loss=0.015, v_num=6, val_loss=16.5, acc=0.0824]                       \n",
      "Epoch 109: 100%|██████████| 312/312 [00:01<00:00, 257.32it/s, loss=0.008, v_num=6, val_loss=16.5, acc=0.0824]\n",
      "Epoch 109: : 317it [00:01, 255.62it/s, loss=0.008, v_num=6, val_loss=18.2, acc=0.08]                         \n",
      "Epoch 114: 100%|██████████| 312/312 [00:01<00:00, 287.91it/s, loss=0.154, v_num=6, val_loss=18.2, acc=0.08]\n",
      "Epoch 114: : 317it [00:01, 285.79it/s, loss=0.154, v_num=6, val_loss=15.2, acc=0.0848]                     \n",
      "Epoch 119: 100%|██████████| 312/312 [00:01<00:00, 255.34it/s, loss=0.007, v_num=6, val_loss=15.2, acc=0.0848]\n",
      "Epoch 119: : 317it [00:01, 254.16it/s, loss=0.007, v_num=6, val_loss=17.5, acc=0.0846]                       \n",
      "Epoch 124: 100%|██████████| 312/312 [00:01<00:00, 265.98it/s, loss=0.004, v_num=6, val_loss=17.5, acc=0.0846]\n",
      "Epoch 124: : 317it [00:01, 258.58it/s, loss=0.004, v_num=6, val_loss=18.8, acc=0.0832]                       \n",
      "Epoch 129: 100%|██████████| 312/312 [00:01<00:00, 257.68it/s, loss=0.002, v_num=6, val_loss=18.8, acc=0.0832]\n",
      "Epoch 129: : 317it [00:01, 256.42it/s, loss=0.002, v_num=6, val_loss=20.1, acc=0.0822]                       \n",
      "Epoch 134: 100%|██████████| 312/312 [00:01<00:00, 255.94it/s, loss=0.001, v_num=6, val_loss=20.1, acc=0.0822]\n",
      "Epoch 134: : 317it [00:01, 254.15it/s, loss=0.001, v_num=6, val_loss=21.6, acc=0.0818]                       \n",
      "Epoch 139: 100%|██████████| 312/312 [00:01<00:00, 270.81it/s, loss=0.009, v_num=6, val_loss=21.6, acc=0.0818]\n",
      "Epoch 139: : 317it [00:01, 269.21it/s, loss=0.009, v_num=6, val_loss=17.5, acc=0.081]                        \n",
      "Epoch 144: 100%|██████████| 312/312 [00:01<00:00, 275.89it/s, loss=0.003, v_num=6, val_loss=17.5, acc=0.081]\n",
      "Epoch 144: : 317it [00:01, 272.58it/s, loss=0.003, v_num=6, val_loss=18.7, acc=0.0816]                      \n",
      "Epoch 149: 100%|██████████| 312/312 [00:01<00:00, 254.89it/s, loss=0.002, v_num=6, val_loss=18.7, acc=0.0816]\n",
      "Epoch 149: : 317it [00:01, 249.10it/s, loss=0.002, v_num=6, val_loss=19.8, acc=0.0818]                       \n",
      "Epoch 149: : 317it [00:01, 244.17it/s, loss=0.002, v_num=6, val_loss=19.8, acc=0.0818]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "data = MinMaxDiffModule(50000, normalize=True)\n",
    "model = MLPModule()\n",
    "trainer = Trainer(check_val_every_n_epoch=5, gpus=1, max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | model        | MLP              | 353 K \n",
      "1 | loss         | CrossEntropyLoss | 0     \n",
      "2 | val_accuracy | Accuracy         | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 13/395 [00:00<00:01, 232.70it/s, loss=2.240, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushaj/miniconda3/envs/dl/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/kushaj/miniconda3/envs/dl/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 390/390 [00:01<00:00, 291.90it/s, loss=0.000, v_num=7]\n",
      "Epoch 4: : 395it [00:01, 288.71it/s, loss=0.000, v_num=7, val_loss=0, acc=1]    \n",
      "Epoch 9: 100%|██████████| 390/390 [00:01<00:00, 262.26it/s, loss=0.000, v_num=7, val_loss=0, acc=1]\n",
      "Epoch 9: : 395it [00:01, 260.79it/s, loss=0.000, v_num=7, val_loss=0, acc=1]                       \n",
      "Epoch 14: 100%|██████████| 390/390 [00:01<00:00, 278.08it/s, loss=0.000, v_num=7, val_loss=0, acc=1]\n",
      "Epoch 14: : 395it [00:01, 276.10it/s, loss=0.000, v_num=7, val_loss=0, acc=1]                       \n",
      "Epoch 19:   6%|▋         | 25/390 [00:00<00:01, 281.53it/s, loss=0.000, v_num=7, val_loss=0, acc=1] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushaj/miniconda3/envs/dl/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
